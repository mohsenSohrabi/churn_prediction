{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c6c408-bcd1-42a7-8edf-7c468b008f4e",
   "metadata": {},
   "source": [
    "# Modeling and Prediction \n",
    "After preprocessing the data we need to load the data, split it, and then create and train the model, and finally make predictions. For this purpose, we will use different models and compare them together. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7cc8a-9fda-4619-868f-05e9b5034956",
   "metadata": {},
   "source": [
    "## Importing neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4f0e05-cc0a-4c55-904f-12abc88a6106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import json\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4291cc-6fd2-4a35-a8a2-620bea7723a9",
   "metadata": {},
   "source": [
    "## Load data \n",
    "In this part, we should load data and extract **`X`** and **`y`** (target which is 'churn') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbac4a11-6399-4381-92fb-4477931a8746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4250, 64), (4250,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('preprocessed_data.csv')\n",
    "X = data.drop(columns=['churn'])\n",
    "y = data['churn']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d4f227-1268-48fa-ab6b-5da4e31d035a",
   "metadata": {},
   "source": [
    "## Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5872e01b-2c43-45de-88b1-3b17a7cbac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c43a89e-59df-4ffb-8974-061f1b027a01",
   "metadata": {},
   "source": [
    "## Models \n",
    "In this section, we will try different models and shows their performance. \n",
    "<ol>\n",
    "    <li> xgboost </li> \n",
    "    <li> SVM </li> \n",
    "    <li> Neural Networks </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad9fae0-590b-4727-8cad-21c74fe04034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide a dictionary to save results in \n",
    "Results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2b0605-e747-4e66-bcb4-9ef9ba33eaf2",
   "metadata": {},
   "source": [
    "### xgboost\n",
    "**XGBoost** is a popular machine learning framework that stands for “**Extreme Gradient Boosting**”. It is an implementation of gradient boosted decision trees designed for speed and performance. This code trains an XGBoost model on a given dataset and evaluates its performance on a test set. Here is a brief explanation of each part of the code:\r\n",
    "\r\n",
    "1. The data is first converted into DMatrix format, which is the internal data structure used by XGBoost. This is done using the `xgb.DMatrix` function, which takes the training data `X_train` and labels `y_train` as input.\r\n",
    "2. The number of rounds for boosting is set to 10 using the `num_round` variable. This means that 10 trees will be built in the XGBoost model.\r\n",
    "3. The parameters for the XGBoost model are set using the `param` dictionary. This includes the maximum depth of the trees (`max_depth`), the learning rate (`eta`), the objective function (`objective`), the number of threads to use for training (`nthread`), and the evaluation metric (`eval_metric`).\r\n",
    "4. A grid of hyperparameters is created using the `ParameterGrid` class from the `sklearn.model_selection` module. This generates all possible combinations of hyperparameters from the `param` dictionary.\r\n",
    "5. The model is trained for each combination of hyperparameters using the `xgb.train` function. This takes the hyperparameters, training data, and number of rounds as input.\r\n",
    "6. Predictions are made on the test set using the `predict` method of the trained model. The predictions are rounded to 0 or 1 using a list comprehension.\r\n",
    "7. The accuracy of the model is evaluated using the `accuracy_score` function from the `sklearn.metrics` module. This compares the predicted labels with the true labels in the test set.\r\n",
    "8. The accuracy and hyperparameters are printed for each combination of hyperparameters.\r\n",
    "\r\n",
    "I hope this helps! Let me know if you have any questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4b8deda-de18-402d-a1ad-f415f524980c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.97 with params: {'eta': 0.3, 'eval_metric': 'auc', 'max_depth': 5, 'nthread': 4, 'objective': 'binary:logistic'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert the data into DMatrix format\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "num_round = 10\n",
    "\n",
    "# Set the parameters for the XGBoost model\n",
    "param = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'eta': [0.1, 0.3, 0.5],\n",
    "    'objective': ['binary:logistic'],\n",
    "    'nthread': [4],\n",
    "    'eval_metric': ['auc']\n",
    "}\n",
    "\n",
    "# Create a grid of hyperparameters\n",
    "param_grid = ParameterGrid(param)\n",
    "\n",
    "# Create a dictionary for storing the results\n",
    "xgb_results = {}\n",
    "\n",
    "# Train the model for each combination of hyperparameters\n",
    "for params in param_grid:\n",
    "    bst = xgb.train(params, dtrain, num_round)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = bst.predict(dtest)\n",
    "    y_pred = [round(y) for y in y_pred]\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store the result in the dictionary\n",
    "    xgb_results[accuracy] = params\n",
    "\n",
    "# Find the result with the best accuracy\n",
    "best_accuracy = max(xgb_results.keys())\n",
    "best_params = xgb_results[best_accuracy]\n",
    "\n",
    "print(f'Best accuracy: {best_accuracy:.2f} with params: {best_params}')\n",
    "with open('xgb_best_params.json','w') as file:\n",
    "    json.dump(best_params,file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0b3b12-929c-46b8-853b-62ffaf92d06f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67c97a25-1f87-46c5-bfbf-832d0d46e3e4",
   "metadata": {},
   "source": [
    "## SVM\n",
    "**SVM** stands for Support Vector Machine. It is a type of supervised machine learning algorithm that can be used for classification or regression tasks. SVMs work by finding the hyperplane that best separates the data into different classes. The hyperplane is chosen to maximize the margin, which is the distance between the hyperplane and the closest data points from each class. These closest data points are called support vectors, hence the name Support Vector Machine.\r\n",
    "\r\n",
    "The code you provided trains an SVM model on a given dataset and evaluates its performance on a test set. Here is a brief explanation of each part of the code:\r\n",
    "\r\n",
    "1. The `SVC` class from the `sklearn.svm` module is imported. This class implements the SVM algorithm for classification.\r\n",
    "2. The `GridSearchCV` class from the `sklearn.model_selection` module is also imported, but it is not used in the code.\r\n",
    "3. The parameters for the SVM model are set using the `param` dictionary. This includes the regularization parameter (`C`), the kernel function (`kernel`), the degree of the polynomial kernel function (`degree`), and the kernel coefficient (`gamma`).\r\n",
    "4. A grid of hyperparameters is created using the `ParameterGrid` class from the `sklearn.model_selection` module. This generates all possible combinations of hyperparameters from the `param` dictionary.\r\n",
    "5. The model is trained for each combination of hyperparameters using the `fit` method of the `SVC` class. This takes the training data and labels as input.\r\n",
    "6. Predictions are made on the test set using the `predict` method of the trained model.\r\n",
    "7. The accuracy of the model is evaluated using the `accuracy_score` function from the `sklearn.metrics` module. This compares the predicted labels with the true labels in the test set.\r\n",
    "8. The accuracy and hyperparameters are printed for each combination of hyperparameters.\r\n",
    "\r\n",
    "I hope this helps! Let me know if you have any questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63d8c349-04a6-44cf-98e0-fd8ec80340fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.91 with params: {'C': 10, 'degree': 4, 'gamma': 'scale', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set the parameters for the SVM model\n",
    "param = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'degree': [2, 3, 4],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Create a grid of hyperparameters\n",
    "param_grid = ParameterGrid(param)\n",
    "\n",
    "# create a dictionary to store best results\n",
    "svm_results = {}\n",
    "\n",
    "# Train the model for each combination of hyperparameters\n",
    "for params in param_grid:\n",
    "    clf = SVC(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    # Store the result in the dictionary\n",
    "    svm_results[accuracy] = params\n",
    "    \n",
    "# Find the result with the best accuracy\n",
    "best_accuracy = max(svm_results.keys())\n",
    "best_params = svm_results[best_accuracy]\n",
    "\n",
    "# print the accuracy and save the params for the best result\n",
    "print(f'Best accuracy: {best_accuracy:.2f} with params: {best_params}')\n",
    "with open('svm_best_params.json','w') as file:\n",
    "    json.dump(best_params,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daffded5-6a5b-4663-a857-9d11fc680b80",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "A neural network is a type of machine learning algorithm that is inspired by the structure and function of the human brain. It consists of layers of interconnected nodes, where each node represents a neuron and the connections between nodes represent synapses. Neural networks can be used for a wide range of tasks, including classification, regression, and clustering.\r\n",
    "\r\n",
    "The code you provided trains a multi-layer perceptron (MLP) neural network on a given dataset and evaluates its performance on a test set. Here is a brief explanation of each part of the code:\r\n",
    "\r\n",
    "1. The `MLPClassifier` class from the `sklearn.neural_network` module is imported. This class implements an MLP neural network for classification.\r\n",
    "2. The `ParameterGrid` class from the `sklearn.model_selection` module is also imported.\r\n",
    "3. The parameters for the neural network are set using the `param` dictionary. This includes the number of neurons in each hidden layer (`hidden_layer_sizes`), the activation function for the neurons (`activation`), the solver for weight optimization (`solver`), the L2 regularization parameter (`alpha`), and the learning rate schedule for weight updates (`learning_rate`).\r\n",
    "4. A grid of hyperparameters is created using the `ParameterGrid` class. This generates all possible combinations of hyperparameters from the `param` dictionary.\r\n",
    "5. The model is trained for each combination of hyperparameters using the `fit` method of the `MLPClassifier` class. This takes the training data and labels as input.\r\n",
    "6. Predictions are made on the test set using the `predict` method of the trained model.\r\n",
    "7. The accuracy of the model is evaluated using the `accuracy_score` function from the `sklearn.metrics` module. This compares the predicted labels with the true labels in the test set.\r\n",
    "8. The accuracy and hyperparameters are printed for each combination of hyperparameters.\r\n",
    "\r\n",
    "I hope this helps! Let me know if you have any questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd3d814-2d71-4f2e-9b78-e69383b9df66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.93 with params: {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 5000, 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters for the neural network\n",
    "param = {\n",
    "    'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "\n",
    "# Create a grid of hyperparameters\n",
    "param_grid = ParameterGrid(param)\n",
    "\n",
    "# Create a dictionary to store results \n",
    "nn_results = {}\n",
    "\n",
    "# Train the model for each combination of hyperparameters\n",
    "for params in param_grid:\n",
    "    clf = MLPClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    nn_results[accuracy] = params\n",
    "\n",
    "\n",
    "\n",
    "# Find the result with the best accuracy\n",
    "best_accuracy = max(nn_results.keys())\n",
    "best_params = nn_results[best_accuracy]\n",
    "\n",
    "# print the accuracy and save the params for the best result\n",
    "print(f'Best accuracy: {best_accuracy:.2f} with params: {best_params}')\n",
    "with open('nn_best_params.json','w') as file:\n",
    "    json.dump(best_params,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd8a35bf-297e-4c93-9223-a1cb22f74580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.97 with params: {'eta': 0.3, 'eval_metric': 'auc', 'max_depth': 5, 'nthread': 4, 'objective': 'binary:logistic'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255084af-67ba-4391-9a0e-c13a91f95d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
